{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56fef95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 09:47:29.533590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-21 09:47:29.533623: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/lib/python3/dist-packages/Crypto/Random/Fortuna/FortunaGenerator.py:28: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if sys.version_info[0] is 2 and  sys.version_info[1] is 1:\n",
      "/usr/lib/python3/dist-packages/Crypto/Random/Fortuna/FortunaGenerator.py:28: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if sys.version_info[0] is 2 and  sys.version_info[1] is 1:\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.get_logger().setLevel('INFO')\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec30b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c9c9b",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c4f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "# IMAGE_SIZE = [1024, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6852bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFRecord Files: 68\n",
      "Validation TFRecord Files: 68\n",
      "Test TFRecord Files: 67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "FILENAMES_PATH = \"American Sign Language Letters.v1-v1.tfrecord/\"\n",
    "\n",
    "TRAINING_FILENAMES =  FILENAMES_PATH + \"train/Letters.tfrecord\"\n",
    "VALID_FILENAMES = FILENAMES_PATH + \"valid/Letters.tfrecord\"\n",
    "TEST_FILENAMES = FILENAMES_PATH + \"test/Letters.tfrecord\"\n",
    "\n",
    "print(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\n",
    "print(\"Validation TFRecord Files:\", len(VALID_FILENAMES))\n",
    "print(\"Test TFRecord Files:\", len(TEST_FILENAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732da708",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = TRAINING_FILENAMES\n",
    "labeled = FILENAMES_PATH + \"/train/Letters_label_map.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dcf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ignore_order = tf.data.Options()\n",
    "# ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "dataset = tf.data.TFRecordDataset(example)\n",
    "# dataset = dataset.with_options(ignore_order)\n",
    "# print(dataset)\n",
    "# raw_example = next(iter(dataset))\n",
    "# print(raw_example)\n",
    "# parsed = tf.train.Example.FromString(raw_example.numpy())\n",
    "# print(parsed)\n",
    "\n",
    "# parsed.features.feature['image/text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dcb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for raw_record in dataset.take(10):\n",
    "#     print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tffeature_description = (\n",
    "{\n",
    "    \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"image/filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"image/format\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"image/object/bbox/xmax\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    \"image/object/bbox/xmin\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    \"image/object/bbox/ymax\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    \"image/object/bbox/ymin\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    \"image/object/class/label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"image/object/class/text\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    ")\n",
    "\n",
    "def _parse_function(example):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "    example_message = tf.io.parse_single_example(example, tffeature_description)\n",
    "    \n",
    "    img_raw = example_message['image/encoded']\n",
    "    height = example_message['image/height']\n",
    "    width = example_message['image/width']\n",
    "    label_no = example_message['image/object/class/label']\n",
    "    label = example_message['image/object/class/text']\n",
    "    return example_message\n",
    "#     feature = tf.io.decode_raw(\n",
    "#         img_raw, out_type=tf.int8, little_endian=True, fixed_length=None, name=None\n",
    "#     )\n",
    "# #     feature = tf.io.parse_tensor(img_raw, out_type=tf.uint8)\n",
    "#     feature = tf.reshape(feature, (width,height))\n",
    "#     return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsed_record in parsed_dataset.take(1):\n",
    "#     print(repr(parsed_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c1392",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = dataset.map(_parse_function)\n",
    "# parsed_dataset\n",
    "for image_features in parsed_dataset.take(1):\n",
    "#     print(image_features)\n",
    "\n",
    "    # Create a TensorProto from serialized_tensor content\n",
    "#     tensor_proto = tf.core.framework.tensor_pb2.TensorProto()\n",
    "#     tensor_proto.ParseFromString(image_features.numpy())\n",
    "    image_raw = image_features['image/encoded'].numpy()\n",
    "#     feature = tf.io.parse_tensor(image_raw, tf.dtypes.as_dtype(tensor_proto.dtype))\n",
    "#     image_raw = tf.make_tensor_proto(image_raw)\n",
    "    print(image_raw)\n",
    "\n",
    "# parsed_img_list = []\n",
    "# for parsed in dataset.take(1):\n",
    "    \n",
    "#     parsed_dataset = _parse_function(parsed)\n",
    "#     parsed_img_list.append(parsed_dataset)\n",
    "    \n",
    "# for image_features in parsed_img_list:\n",
    "#     image_raw = image_features['image/encoded'].numpy()\n",
    "    \n",
    "#     image_h = image_features[\"image/height\"].numpy()\n",
    "#     image_w = image_features[\"image/width\"].numpy()\n",
    "#     print(image_h)\n",
    "#     print(image_w)\n",
    "#     display.display(display.Image(data=image_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding function\n",
    "def parse_record(record):\n",
    "    tfrecord_feat_format = (\n",
    "                {\n",
    "                    \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n",
    "                    \"image/filename\": tf.io.FixedLenFeature([], tf.string),\n",
    "                    \"image/format\": tf.io.FixedLenFeature([], tf.string),\n",
    "                    \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
    "                    \"image/object/bbox/xmax\": tf.io.FixedLenFeature([], tf.float32),\n",
    "                    \"image/object/bbox/xmin\": tf.io.FixedLenFeature([], tf.float32),\n",
    "                    \"image/object/bbox/ymax\": tf.io.FixedLenFeature([], tf.float32),\n",
    "                    \"image/object/bbox/ymin\": tf.io.FixedLenFeature([], tf.float32),\n",
    "                    \"image/object/class/label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "                    \"image/object/class/text\": tf.io.FixedLenFeature([], tf.string),\n",
    "                    \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    \n",
    "    \n",
    "    example_message = tf.io.parse_single_example(record, tfrecord_feat_format)\n",
    "        \n",
    "        \n",
    "    img_raw = example_message['image/encoded']\n",
    "    height = example_message['image/height']\n",
    "    width = example_message['image/width']\n",
    "    label_no = example_message['image/object/class/label']\n",
    "    label = example_message['image/object/class/text']\n",
    "    \n",
    "\n",
    "    feature = tf.io.parse_tensor(img_raw, out_type=tf.uint8)\n",
    "    feature = tf.reshape(feature, (width,height))\n",
    "    \n",
    "    \n",
    "    return (feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset object for tfrecord file(s)\n",
    "\n",
    "def load_dataset(tf_filenames, set_type):\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "    dataset = tf.data.TFRecordDataset(tf_filenames) \n",
    "    \n",
    "    dataset = dataset.with_options(ignore_order)  \n",
    "\n",
    "    dataset = dataset.map(parse_record, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.shuffle(128, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.repeat() if set_type =='train' else dataset \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb43f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(TRAINING_FILENAMES, \"train\")\n",
    "    \n",
    "for sample in dataset.take(1):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3494b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode_tfrecord(record):\n",
    "#     image = tf.io.decode_raw(\n",
    "#         record[\"image/encoded\"], out_type=tf.int8, little_endian=True, fixed_length=None, name=None\n",
    "#     )\n",
    "#     label = record[\"image/object/class/text\"]\n",
    "# #     image = tf.reshape(image, (dimension,dimension))\n",
    "#     return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_list = []\n",
    "# n_samples_to_show = 16\n",
    "# c = 0\n",
    "\n",
    "# dataset = load_dataset(example)\n",
    "# # print(dataset)\n",
    "# for record in dataset:\n",
    "# #     print(dataset)\n",
    "#     c+=1\n",
    "#     if c > n_samples_to_show:\n",
    "#         break\n",
    "#     parsed_record = parse_record(record)\n",
    "#     decoded_record = decode_tfrecord(parsed_record)\n",
    "#     image,label = decoded_record\n",
    "#     img_list.append(image)\n",
    "\n",
    "# # Visualization\n",
    "# for image in img_list:\n",
    "#     display.display(display.Image(data=image))\n",
    "    \n",
    "# or it can be visualized in a grid format\n",
    "\n",
    "# from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "# fig = plt.figure(figsize=(20,20))\n",
    "# grid = ImageGrid(fig, 111, nrows_ncols=(4,4), axes_pad=0.1)\n",
    "\n",
    "# # show image grid\n",
    "# for ax, im in zip(grid, img_list):\n",
    "# #     Iterating over the grid returns the Axes\n",
    "#     ax.imshow(im, \"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d073640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S19_jpg.rf.f13dfc282ee341b185a0c7ad26888cbb.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "count=0\n",
    "images = []\n",
    "annotations = []\n",
    "for files in os.listdir(TRAINING_FILENAMES):\n",
    "#     count+= 1\n",
    "# print(count)\n",
    "\n",
    "    if \"csv\" in files:\n",
    "        annotations.append(files)\n",
    "\n",
    "    else:\n",
    "        images.append(files)\n",
    "\n",
    "# print(annotations)\n",
    "print(images[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a7f44de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Sign Language Letters.v1-v1.tensorflow/train/S19_jpg.rf.f13dfc282ee341b185a0c7ad26888cbb.jpg\n",
      "JPEG\n",
      "(406, 406)\n",
      "RGB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 494508 into shape (400,400,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5085/3080150319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# image = np.fromstring(image).reshape((400, 400, 3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# image_raw = image.tostring()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 494508 into shape (400,400,3)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "  \n",
    "from numpy import asarray\n",
    "\n",
    "\n",
    "img = TRAINING_FILENAMES + \"/\" + images[1]\n",
    "image = Image.open(img)\n",
    "print(img)\n",
    "# print(image)\n",
    "print(image.format)\n",
    "print(image.size)\n",
    "print(image.mode)\n",
    "\n",
    "\n",
    "# image = np.fromstring(image).reshape((400, 400, 3))\n",
    "print(asarray(image).reshape((400, 400, 3)))\n",
    "\n",
    "# image_raw = image.tostring()\n",
    "# print(image_raw)\n",
    "\n",
    "\n",
    "# from IPython.display import Image\n",
    "# Image(filename=img, width = 300, height = 300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f37433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       20\n",
      "1       15\n",
      "2       10\n",
      "3       22\n",
      "4        9\n",
      "        ..\n",
      "1507     2\n",
      "1508     0\n",
      "1509     0\n",
      "1510    22\n",
      "1511    18\n",
      "Name: label, Length: 1512, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nwoke/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = pd.read_csv(os.path.join(TRAINING_FILENAMES, annotations[0]))\n",
    "lb_make = LabelEncoder()\n",
    "dataset['label'] = lb_make.fit_transform(dataset[[\"class\"]])\n",
    "\n",
    "print(dataset['label'])\n",
    "\n",
    "# dataset[[\"label\", \"class\"]].head(100)\n",
    "\n",
    "# dataset = dataset.to_dict(orient='records')\n",
    "# .apply(lambda x: x.to_json(r'annotations.json'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b920b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc0a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f40bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b37b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21934fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a20a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18456382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2de4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILENAMES_PATH = \"/home/nwoke/Documents/git_cloned/Github/MLflow_TF-serving/data_store/data/American Sign Language Letters.v1-v1.tensorflow/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c74d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFRecord Files: 53\n",
      "Validation TFRecord Files: 53\n",
      "Test TFRecord Files: 52\n"
     ]
    }
   ],
   "source": [
    "TRAINING_FILENAMES =  FILENAMES_PATH + \"train/\"\n",
    "VALID_FILENAMES = FILENAMES_PATH + \"valid\"\n",
    "TEST_FILENAMES = FILENAMES_PATH + \"test\"\n",
    "\n",
    "print(\"Train TFRecord Files:\", len(TRAINING_FILENAMES))\n",
    "print(\"Validation TFRecord Files:\", len(VALID_FILENAMES))\n",
    "print(\"Test TFRecord Files:\", len(TEST_FILENAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8750e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv, json\n",
    "import pandas as pd \n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c647e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n",
    "    )\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()]))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def float_feature_list(value):\n",
    "    \"\"\"Returns a list of float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebcd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, path, example):\n",
    "    feature = {\n",
    "        \"image/encoded\": image_feature(image),\n",
    "        \"image/filename\": bytes_feature(example[\"filename\"]),\n",
    "        \"image/format\": bytes_feature(\"jpg\"),\n",
    "        \"image/height\":  int64_feature(example[\"height\"]),\n",
    "        \"image/object/bbox/xmin\": float_feature(example[\"xmin\"]),        \n",
    "        \"image/object/bbox/ymin\": float_feature(example[\"ymin\"]),\n",
    "        \"image/object/bbox/xmax\": float_feature(example[\"xmax\"]),\n",
    "        \"image/object/bbox/ymax\": float_feature(example[\"ymax\"]),\n",
    "        \"image/object/class/label\": int64_feature(example[\"label\"]),\n",
    "        \"image/object/class/text\": bytes_feature(example[\"class\"]),\n",
    "        \"image/width\": int64_feature(example[\"width\"])\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99dd8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_rec_writer(dataset_, path, data_split):\n",
    "    record_file = f\"{data_split}.tfrecords\"\n",
    "    with tf.io.TFRecordWriter(record_file) as writer: \n",
    "        for data in dataset_:\n",
    "\n",
    "            image = os.path.join(path, data['filename'])\n",
    "\n",
    "            image = tf.io.decode_jpeg(tf.io.read_file(image))\n",
    "    #         print(image)\n",
    "            feature = create_example(image, path, data)\n",
    "            writer.write(feature.SerializeToString())\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1a10fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_loader(FILE_PATH):\n",
    "    count=0\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for files in os.listdir(FILE_PATH):\n",
    "    #     count+= 1\n",
    "    # print(count)\n",
    "\n",
    "        if \"csv\" in files:\n",
    "            annotations.append(files)\n",
    "\n",
    "        else:\n",
    "            images.append(files)\n",
    "\n",
    "    # print(annotations)\n",
    "#     print(images)\n",
    "    return annotations, images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e87d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(FILE_PATH, annotations):\n",
    "    dataset = pd.read_csv(os.path.join(FILE_PATH, annotations[0]))\n",
    "    lb_make = LabelEncoder()\n",
    "    dataset['label'] = lb_make.fit_transform(dataset[[\"class\"]])\n",
    "\n",
    "    dataset[[\"label\", \"class\"]].head(100)\n",
    "\n",
    "    dataset = dataset.to_dict(orient='records')\n",
    "    return dataset\n",
    "# .apply(lambda x: x.to_json(r'annotations.json'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b9d5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(os.path.join(TRAINING_FILENAMES, annotations[0]))\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f93053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[1]\n",
    "# dataset[filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba394db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'American Sign Language Letters.v1-v1.tensorflow/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m annotations_train, images \u001b[38;5;241m=\u001b[39m \u001b[43mfile_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAINING_FILENAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m convert_to_dict(TRAINING_FILENAMES, annotations_train)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfile_loader\u001b[0;34m(FILE_PATH)\u001b[0m\n\u001b[1;32m      3\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m annotations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m files \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE_PATH\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     count+= 1\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(count)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     10\u001b[0m         annotations\u001b[38;5;241m.\u001b[39mappend(files)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'American Sign Language Letters.v1-v1.tensorflow/train'"
     ]
    }
   ],
   "source": [
    "annotations_train, images = file_loader(TRAINING_FILENAMES)\n",
    "dataset_train = convert_to_dict(TRAINING_FILENAMES, annotations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c748a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_test, images = file_loader(TEST_FILENAMES)\n",
    "dataset_test = convert_to_dict(TEST_FILENAMES, annotations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6483b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_valid, images = file_loader(VALID_FILENAMES)\n",
    "dataset_valid = convert_to_dict(VALID_FILENAMES, annotations_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a51e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 13:47:21.763996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2020.1.023/opencv/lib:/opt/intel/openvino_2020.1.023/deployment_tools/ngraph/lib:/opt/intel/opencl:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/external/tbb/lib:/opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64:\n",
      "2022-04-05 13:47:21.764182: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-05 13:47:21.764224: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Tobiee): /proc/driver/nvidia/version does not exist\n",
      "2022-04-05 13:47:21.764694: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_tfrec = tf_rec_writer(dataset_train, TRAINING_FILENAMES, \"train\")\n",
    "test_tfrec = tf_rec_writer(dataset_test, TEST_FILENAMES, \"test\")\n",
    "valid_tfrec = tf_rec_writer(dataset_valid, VALID_FILENAMES, \"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e791db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
